{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Demographic Parity and Equalized Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook I created while reading [Equality of Opportunity in Supervised Learning](https://arxiv.org/abs/1610.02413). \n",
    "\n",
    "I am not sure there is a way to access the code the authors used to produce their plots/results, however, after some googling I found the [Fairlearn](https://fairlearn.github.io/v0.5.0/index.html#) package, which seemingly aims to _empowers developers of artificial intelligence (AI) systems to assess their system's fairness and mitigate any observed unfairness issues_. Reading their documentation they seem to implement, among others, the methodology described by the paper above.\n",
    "\n",
    "I thought it would be good to play around with this tool to assimilate the theoretical concepts introduced by the paper, so what you find below is a minimal overview of the practical use of this methodology, compared to another standard definition of fairness called _Demographic Parity_ (also implemented in Fairlearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_openml(data_id=1590, as_frame=True)\n",
    "X = pd.get_dummies(data.data)\n",
    "y_true = (data.target == '>50K') * 1\n",
    "sex = data.data['sex']\n",
    "sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import MetricFrame\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(min_samples_leaf=10, max_depth=4)\n",
    "classifier.fit(X, y_true)\n",
    "y_pred = classifier.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No fairness measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = MetricFrame(accuracy_score, y_true, y_pred, sensitive_features=sex)\n",
    "print(f\"Total accuracy score: {gm.overall}\\n\")\n",
    "print(f\"Accuracy score by: {gm.by_group}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness metrics\n",
    "\n",
    "When working on fairness, before we can start _fairifying_ models we need to establish fairness metrics to optimise. Here we consider two definitions _Demographic Parity_ and _Equalized Odds_.\n",
    "\n",
    "* **Demographic Parity**: is defined as `Pr(Y_pred=1 | A=1) = Pr(Y_pred=1 | A=0)`\n",
    "* **Equalized Odds**: is defined as `Pr(Y_pred=1 | A=1, Y=y) = Pr(Y_pred=1 | A=0, Y=y)`\n",
    "\n",
    "Since in both cases we won't get perfect equality a good way to assess fairness is to look at the difference between the terms, and this is exactly what is done in `fairnlearn`:\n",
    "\n",
    "For _Demographic Parity_ we have:\n",
    "```\n",
    "demographic_parity_difference = |Pr(Y_pred=1 | A=1) - Pr(Y_pred=1 | A=0)|\n",
    "```\n",
    "\n",
    "For _Equalized Odds_ we have:\n",
    "```\n",
    "equalized_odds_difference = max(\n",
    "    |Pr(Y_pred=1 | A=1, Y=0) - Pr(Y_pred=1 | A=0, Y=0)|,\n",
    "    |Pr(Y_pred=1 | A=1, Y=1) - Pr(Y_pred=1 | A=0, Y=1)|\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n",
    "\n",
    "dp_difference = demographic_parity_difference(y_true, y_pred, sensitive_features=sex)\n",
    "print(f\"Demographic parity difference: {dp_difference}\")\n",
    "\n",
    "eo_difference = equalized_odds_difference(y_true, y_pred, sensitive_features=sex)\n",
    "print(f\"Equalized odds difference: {eo_difference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "\n",
    "mitigator = ExponentiatedGradient(classifier, DemographicParity())\n",
    "mitigator.fit(X, y_true, sensitive_features=sex)\n",
    "y_pred_mitigated = mitigator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_dp = MetricFrame(accuracy_score, y_true, y_pred_mitigated, sensitive_features=sex)\n",
    "print(f\"Total accuracy score {gm_dp.overall}\")\n",
    "print(gm_dp.by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_difference = demographic_parity_difference(y_true, y_pred_mitigated, sensitive_features=sex)\n",
    "print(f\"Demographic parity difference: {dp_difference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.reductions import EqualizedOdds\n",
    "\n",
    "mitigator = ThresholdOptimizer(estimator=classifier, constraints='equalized_odds')\n",
    "mitigator.fit(X, y_true, sensitive_features=sex)\n",
    "y_pred_mitigated = mitigator.predict(X, sensitive_features=sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_eo = MetricFrame(accuracy_score, y_true, y_pred_mitigated, sensitive_features=sex)\n",
    "print(f\"Total accuracy score {gm_eo.overall}\")\n",
    "print(gm_eo.by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eo_difference = equalized_odds_difference(y_true, y_pred_mitigated, sensitive_features=sex)\n",
    "print(f\"Equalized odds difference: {eo_difference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
